{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "plt.style.use('ggplot')\n",
    "red = mpl.rcParams['axes.prop_cycle'].by_key()['color'][0]\n",
    "blue = mpl.rcParams['axes.prop_cycle'].by_key()['color'][1]\n",
    "purple = mpl.rcParams['axes.prop_cycle'].by_key()['color'][2]\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import  accuracy_score, balanced_accuracy_score, roc_auc_score, f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group fairness' robustness on *adult income* dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we work with the *adult income* dataset. We start by loading and processing the data. Exploratory data analysis for the dataset can be found in *[eda_income](notebooks/eda_income.ipynb)*. We then create a baseline, evaluate the performance, fairness and feature importance of the model. Afterwards, we repeat but with a fairness constrained model, constrained on being fair for gender. We then similarly evaluate it, and compare the two models. Lastly, we scale the experiment to being fair for gender, age and race, to see how the method works in a realistic classification setting.\n",
    "\n",
    "Start by loading in the data and doing some initial processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income = pd.read_csv('data/income.csv')\n",
    "income['native-country'] = income['native-country'].apply(lambda x: x if x == 'United-States' else 'Other')\n",
    "income['target'] = income.income.map({'<=50K': 0, '>50K': 1})\n",
    "\n",
    "income['gender_group'] = income.gender.map({'Male': 0, 'Female': 1})\n",
    "income['race_group'] = income['race'].apply(lambda x: 1 if x == 'White' else 0)\n",
    "income['age_group'] = income['age'].apply(lambda x: 1 if x < 35 else 0)\n",
    "\n",
    "income.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define which columns are going to be used for classification, whether they're going to get categorical or continuous preprocessing, and the groups used for debiasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'native-country']\n",
    "num_features = ['educational-num']\n",
    "target = 'target'\n",
    "\n",
    "groups = ['gender_group', 'race_group', 'age_group']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our preprocessing pipeline, we one-hot encode categorical features and standardscale all numerical features. Normally, when onehot encoding, we'd drop first for interpretability. However, since we're doing cross validation, we encode all values as features, and drop any features that are uknown in the validation set. Additionally, normally, we'd split the dataset into train and test. However, here, we're just interested in evaluating the robustness of the methods, and as such, we strictly use cross validation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = ColumnTransformer(\n",
    "    [('onehotencoder', OneHotEncoder(handle_unknown='ignore'), nom_features),\n",
    "     ('standardscaler', StandardScaler(), num_features)\n",
    "     ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the project, we must evaluate the fairness of models. For this, we need to define which fairness criteria to fulfill. For this project, since we're using a group fairness constraint, we evaluate the fairness through *equalized odds*. It specifies the true positive ratio and false positive ratio for 2 groups must be equivalent.\n",
    "\n",
    "$$\n",
    "P(R=+|Y=y, A=a) = P(R=+|Y=y, A=b) \\; y \\in \\{+, -\\} \\; \\forall a,b \\in A\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalized_odds(y_true, y_pred, groups):\n",
    "    mask = groups == 0\n",
    "    return y_pred[mask & (y_true == 1)].mean(), y_pred[~mask & (y_true == 1)].mean(), y_pred[mask & (y_true == 0)].mean(), y_pred[~mask & (y_true == 0)].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "For future reference, we start by making a baseline model without any fairness considerations to see what the performance and fairness of the model looks like.\n",
    "\n",
    "The baseline is a binary logistic regression model. We implement it in pytorch as a single feed forward layer followed by sigmoid. We also implement a fit method for training the classifier. Since the original paper we base our research on uses l2-regularization, we also use it with regularization strength $\\lambda = 0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(in_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x)).flatten()\n",
    "    \n",
    "    def fit(self, x, y, lr=0.01, weight_decay=0.01, batch_size=128, n_iter=10):\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        trainloader = DataLoader(TensorDataset(x, y), batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        for epoch in range(n_iter):\n",
    "            for x, y in trainloader:\n",
    "                y_pred = self(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easy tracking across models and k-folds, we implement a *KfoldTracker* to track performance, fairness and feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KfoldTracker:\n",
    "    def __init__(self):\n",
    "        self.performance = defaultdict(list)\n",
    "        self.fairness = defaultdict(list)\n",
    "        self.feature_importance = defaultdict(list)\n",
    "        self.group_tracker = defaultdict(KfoldTracker)\n",
    "\n",
    "    def log_performance(self, y_true, y_pred):\n",
    "        self.performance['Accuracy'].append(accuracy_score(y_true, y_pred))\n",
    "        self.performance['Balanced accuracy'].append(balanced_accuracy_score(y_true, y_pred))\n",
    "        self.performance['f1-score'].append(f1_score(y_true, y_pred))\n",
    "        self.performance['ROC AUC'].append(roc_auc_score(y_true, y_pred))\n",
    "\n",
    "    def log_fairness(self, y_true, y_pred, groups):\n",
    "        for i in range(groups.shape[1]):\n",
    "            tpr_g1, tpr_g2, fpr_g1, fpr_g2 = equalized_odds(y_true, y_pred, groups[:, i])\n",
    "            self.group_tracker[f'group_{i}'].fairness['TPR G1'].append(tpr_g1)\n",
    "            self.group_tracker[f'group_{i}'].fairness['TPR G2'].append(tpr_g2)\n",
    "            self.group_tracker[f'group_{i}'].fairness['FPR G1'].append(fpr_g1)\n",
    "            self.group_tracker[f'group_{i}'].fairness['FPR G2'].append(fpr_g2)\n",
    "\n",
    "    def log_feature_importance(self, feature_importance, feature_names):\n",
    "        for i, name in enumerate(feature_names):\n",
    "            self.feature_importance[name].append(feature_importance[i])\n",
    "        for key in self.feature_importance.keys():\n",
    "            if key not in feature_names:\n",
    "                self.feature_importance[key].append(np.nan)\n",
    "\n",
    "    def log(self, y_true, y_pred, groups, feature_importance, feature_names):\n",
    "        self.log_performance(y_true, y_pred)\n",
    "        self.log_fairness(y_true, y_pred, groups)\n",
    "        self.log_feature_importance(feature_importance, feature_names)\n",
    "\n",
    "    def get_mean_performance(self):\n",
    "        return {k: (np.mean(v), np.std(v)) for k, v in self.performance.items()}\n",
    "\n",
    "    def get_mean_group_performance(self, group_name):\n",
    "        return {k: (np.mean(v), np.std(v)) for k, v in self.group_performance[group_name].items()}\n",
    "    \n",
    "    def get_mean_fairness(self):\n",
    "        return {k: (np.mean(v), np.std(v)) for k, v in self.fairness.items()}\n",
    "    \n",
    "    def get_mean_feature_importance(self):\n",
    "        return {k: (np.mean(v), np.std(v)) for k, v in self.feature_importance.items()}\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(pd.DataFrame(self.performance).T) + '\\n\\n' + str(pd.DataFrame(self.fairness).T) + '\\n\\n' + str(pd.DataFrame(self.feature_importance).T)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.group_tracker[key]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the baseline classifier with k-fold cross validation with $k=10$. This allows us to evaluate the models performance on different train-validation splits, and the uncertainty associated with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_base = KfoldTracker()\n",
    "\n",
    "n_splits = 10\n",
    "splits = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(income)):\n",
    "    train = income.iloc[train_idx]\n",
    "    val = income.iloc[val_idx]\n",
    "    \n",
    "    x_train = preprocessing.fit_transform(train).toarray()\n",
    "    y_train = train[target].values\n",
    "    g_train = train[groups].values\n",
    "    \n",
    "    x_val = preprocessing.transform(val).toarray()\n",
    "    y_val = val[target].values\n",
    "    g_val = val[groups].values\n",
    "\n",
    "    x_train, y_train, g_train, x_val, y_val, g_val = map(lambda x: torch.tensor(x, dtype=torch.float), (x_train, y_train, g_train, x_val, y_val, g_val))\n",
    "\n",
    "    model = BinaryLogisticRegression(x_train.shape[1])\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    y_pred = model(x_val)\n",
    "    y_pred = y_pred.detach().numpy().round()\n",
    "\n",
    "    feature_importance = torch.exp(model.linear.weight).detach().numpy().squeeze()    \n",
    "    feature_names = preprocessing.get_feature_names_out()\n",
    "    tracker_base.log(y_val, y_pred, g_val, feature_importance, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_base = tracker_base.get_mean_performance()\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(perf_base.keys(), [v[0] for v in perf_base.values()], yerr=[v[1] for v in perf_base.values()])\n",
    "ax.set_title('Adult income - baseline model\\nPerformance')\n",
    "ax.set_ylabel('Score')\n",
    "\n",
    "FIG_PATH = 'reports/figs/income/'\n",
    "def savefig(fname, dpi=None):\n",
    "    dpi = 150 if dpi == None else dpi\n",
    "    plt.savefig(os.path.join(FIG_PATH, fname), bbox_inches='tight', dpi=dpi)\n",
    "    \n",
    "savefig('baseline_performance.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outcomes(tracker):\n",
    "    fig, axs = plt.subplots(1, 3, sharey=True, figsize=(10, 6))\n",
    "    for i in range(len(groups)):\n",
    "        fairness = tracker[f'group_{i}'].get_mean_fairness()\n",
    "        axs[i].bar(fairness.keys(), [v[0] for v in fairness.values()], yerr=[v[1] for v in fairness.values()])\n",
    "        axs[i].set_title(f'{groups[i]}'.replace('_', ' '))\n",
    "    axs[0].set_ylabel('Ratio')\n",
    "    return fig, axs\n",
    "\n",
    "fig, axs = plot_outcomes(tracker_base)\n",
    "\n",
    "fig.suptitle('Adult income - baseline model\\nClassification outcomes')\n",
    "savefig('baseline_fairness.svg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate feature importance across all k-folds, and use the multiple folds for estimating the uncertainty of the model. Since we're using a logistic regression model, we can calculate the odds ratio between $x$ and $x'$, where $x$ and $x'$ are identical except for one feature, as:\n",
    "\n",
    "$$\n",
    "\\frac{odds(x')}{odds(x)} = e^{\\beta_j}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(feature_importance):\n",
    "\n",
    "    means = np.array([value[0] for value in feature_importance.values()])\n",
    "    stds = np.array([value[1] for value in feature_importance.values()])\n",
    "    keys = np.array(list(feature_importance.keys()))\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "    fig.subplots_adjust(hspace=0.05)\n",
    "    idx = np.argsort(means)\n",
    "    idx = np.concatenate((idx[:10], idx[-10:]))\n",
    "\n",
    "    ax1.barh(keys[idx[10:]], means[idx[10:]], xerr=stds[idx[10:]])\n",
    "    ax2.barh(keys[idx[:10]], means[idx[:10]], xerr=stds[idx[:10]])\n",
    "\n",
    "    ax1.xaxis.tick_top()\n",
    "\n",
    "    d = .5\n",
    "    kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12,\n",
    "                linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "    ax1.plot([0, 1], [0, 0], transform=ax1.transAxes, **kwargs)\n",
    "    ax2.plot([0, 1], [1, 1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "    return fig, (ax1, ax2)\n",
    "\n",
    "feature_importance_base = tracker_base.get_mean_feature_importance()\n",
    "fig, (ax1, ax2) = plot_feature_importance(feature_importance_base)\n",
    "ax2.set_xlabel('Feature importance')\n",
    "fig.suptitle('Adult income - baseline model\\nFeature importance for income > 50k')\n",
    "savefig('baseline_feature_importance.svg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fair model - enforcing fairness through constrained optimization\n",
    "\n",
    "To debias models, we implement the group fairness constraint from:\n",
    "\n",
    "Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth\n",
    "Neel, and Aaron Roth. A convex framework for fair regression. arXiv preprint arXiv:1706.02409, 2017.\n",
    "\n",
    "$$\n",
    "f_2(\\mathbf{w}, S)=\\left(\\frac{1}{n_1 n_2} \\sum_{\\substack{\\left(\\mathbf{x}_i, y_i\\right) \\in S_1 \\\\\\left(\\mathbf{x}_j, y_j\\right) \\in S_2}} d\\left(y_i, y_j\\right)\\left(\\mathbf{w} \\cdot \\mathbf{x}_i-\\mathbf{w} \\cdot \\mathbf{x}_j\\right)\\right)^2\n",
    "$$\n",
    "\n",
    "This cost function does not work in our setting. We initially tested with it, but we experienced that it didn't work for us (for more info, see *[true_constraint.ipynb](notebooks/true_constraint.ipynb)*). Instead, we let *g* be our decision function $g(x) = \\frac{1}{1 + e^{-(wx)}}$, and implement the loss as:\n",
    "\n",
    "$$\n",
    "f_2(\\mathbf{w}, S)=\\frac{1}{n_1 n_2} \\left(\\sum_{\\substack{\\left(\\mathbf{x}_i, y_i\\right) \\in S_1 \\\\\\left(\\mathbf{x}_j, y_j\\right) \\in S_2}} d\\left(y_i, y_j\\right)\\left(g(\\mathbf{x}_i) - g(\\mathbf{x}_j) \\right)\\right)^2\n",
    "$$\n",
    "\n",
    "We implement this as an optimization constrain for our fair model, such that we use it as a regulerization term, with parameter $\\gamma$ used as regulerization strength. As such, the loss function we're optimizing becomes.\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\frac{1}{N} \\left[ \\sum_{i=1}^N y_i \\; log \\, g_\\beta(\\mathbf{x}_i) + (1 - y_i) \\; log \\, (1-g_\\beta(\\mathbf{x}_i)) \\right] \n",
    "+ \n",
    "\\gamma \\left( \\frac{1}{n_1 n_2} \\left(\\sum_{\\substack{\\left(\\mathbf{x}_i, y_i\\right) \\in S_1 \\\\\\left(\\mathbf{x}_j, y_j\\right) \\in S_2}} d\\left(y_i, y_j\\right)\\left( g_\\beta(\\mathbf{x}_i) - g_\\beta(\\mathbf{x}_j) \\right)\\right)^2 \\right) + \\lambda ||\\beta||_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupFairnessConstraint(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GroupFairnessConstraint, self).__init__()\n",
    "        \n",
    "    def forward(self, y_pred, y_true, groups):\n",
    "        mask = groups == 0\n",
    "        S_pred_1, S_true_1 = y_pred[mask],  y_true[mask]\n",
    "        S_pred_2, S_true_2 = y_pred[~mask], y_true[~mask]\n",
    "        \n",
    "        n_1 = S_pred_1.shape[0]\n",
    "        n_2 = S_pred_2.shape[0]\n",
    "\n",
    "        idx1 = torch.arange(n_1)\n",
    "        idx2 = torch.arange(n_2)\n",
    "\n",
    "        idx1 = idx1.view(-1, 1)\n",
    "        idx2 = idx2.view(1, -1)\n",
    "\n",
    "        idx1 = idx1.repeat(1, idx2.size(1))\n",
    "        idx2 = idx2.repeat(idx1.size(0), 1)\n",
    "\n",
    "        result = torch.stack((idx1, idx2), dim=2).view(-1, 2)\n",
    "        idx1 = result[:, 0]\n",
    "        idx2 = result[:, 1]\n",
    "        \n",
    "        vector = self.d(S_true_1[idx1], S_true_2[idx2]) * (S_pred_1[idx1] - S_pred_2[idx2])\n",
    "        cost = vector.sum()\n",
    "        cost = cost ** 2\n",
    "        cost = cost / (n_1*n_2)\n",
    "\n",
    "        return cost\n",
    "        \n",
    "    def d(self, y_i, y_j):\n",
    "        return (y_i == y_j).int()\n",
    "    \n",
    "class FairBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    def __init__(self, in_dim: int, gamma: float = 1e-4):\n",
    "        super().__init__(in_dim)\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def fit(self, x, y, g, lr=0.01, weight_decay=0.01, batch_size=128, n_iter=10, verbose=False):\n",
    "        criterion = nn.BCELoss()\n",
    "        fair_criterion = GroupFairnessConstraint()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        trainloader = DataLoader(TensorDataset(x, y, g), batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        for epoch in range(n_iter):\n",
    "            for x, y, g in trainloader:\n",
    "                y_pred = self(x)\n",
    "                loss_bce = criterion(y_pred, y)\n",
    "                loss_fair = self.gamma * fair_criterion(y_pred, y, g)\n",
    "                loss = loss_bce + loss_fair\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if verbose:\n",
    "                    print(f'Epoch: {epoch}, Loss BCE: {round(loss_bce.item(), 4)}, Loss Fair: {round(loss_fair.item(), 4)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement a gridsearch for finding the optimal $\\gamma$ for the optimal accuracy-fairness tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammaspace = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1]\n",
    "trackers = []\n",
    "\n",
    "for gamma in gammaspace:\n",
    "    tracker = KfoldTracker()\n",
    "\n",
    "    n_splits = 5\n",
    "    splits = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    for fold, (train_idx, val_idx) in enumerate(splits.split(income)):\n",
    "        train = income.iloc[train_idx]\n",
    "        val = income.iloc[val_idx]\n",
    "        \n",
    "        x_train = preprocessing.fit_transform(train).toarray()\n",
    "        y_train = train[target].values\n",
    "        g_train = train[groups].values\n",
    "        \n",
    "        x_val = preprocessing.transform(val).toarray()\n",
    "        y_val = val[target].values\n",
    "        g_val = val[groups].values\n",
    "\n",
    "        x_train, y_train, g_train, x_val, y_val, g_val = map(lambda x: torch.tensor(x, dtype=torch.float), (x_train, y_train, g_train, x_val, y_val, g_val))\n",
    "\n",
    "        model = FairBinaryLogisticRegression(x_train.shape[1], gamma=gamma)\n",
    "        model.fit(x_train, y_train, g_train[:, 0])\n",
    "        \n",
    "        y_pred = model(x_val)\n",
    "        y_pred = y_pred.detach().numpy().round()\n",
    "\n",
    "        feature_importance = torch.exp(model.linear.weight).detach().numpy().squeeze()    \n",
    "        feature_names = preprocessing.get_feature_names_out()\n",
    "        tracker.log(y_val, y_pred, g_val, feature_importance, feature_names)\n",
    "\n",
    "        mask = g_val[:, 0] == 0\n",
    "        tracker['male'].log_performance(y_val[mask], y_pred[mask])\n",
    "        tracker['female'].log_performance(y_val[~mask], y_pred[~mask])\n",
    "\n",
    "    trackers.append(tracker)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the gamma search in pareto curves to find the optimal gamma value. We plot the f1-score for each group along with the true positive ratios so we can identify the point of equalized opportunity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_perf = np.array([t['male'].get_mean_performance()['f1-score'][0] for t in trackers])\n",
    "female_perf = np.array([t['female'].get_mean_performance()['f1-score'][0] for t in trackers])\n",
    "\n",
    "male_unc = np.array([t['male'].get_mean_performance()['f1-score'][1] for t in trackers])\n",
    "female_unc = np.array([t['female'].get_mean_performance()['f1-score'][1] for t in trackers])\n",
    "\n",
    "tpr_g1 = np.array([t['group_0'].get_mean_fairness()['TPR G1'][0] for t in trackers])\n",
    "tpr_g2 = np.array([t['group_0'].get_mean_fairness()['TPR G2'][0] for t in trackers])\n",
    "tpr_g1_unc = np.array([t['group_0'].get_mean_fairness()['TPR G1'][1] for t in trackers])\n",
    "tpr_g2_unc = np.array([t['group_0'].get_mean_fairness()['TPR G2'][1] for t in trackers])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "ax1.plot(gammaspace, male_perf, label='male')\n",
    "ax1.plot(gammaspace, female_perf, label='female')\n",
    "\n",
    "ax1.fill_between(gammaspace, male_perf + male_unc, male_perf - male_unc, alpha=0.2)\n",
    "ax1.fill_between(gammaspace, female_perf + female_unc, female_perf - female_unc, alpha=0.2)\n",
    "\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "ax1.set_ylabel('F1-score')\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "ax1.set_title('Adult income - gender constrained model\\nPareto curves for different values of gamma')\n",
    "\n",
    "ax2.plot(gammaspace, tpr_g1, label='TPR G1')\n",
    "ax2.plot(gammaspace, tpr_g2, label='TPR G2')\n",
    "\n",
    "ax2.fill_between(gammaspace, tpr_g1 + tpr_g1_unc, tpr_g1 - tpr_g1_unc, alpha=0.2)\n",
    "ax2.fill_between(gammaspace, tpr_g2 + tpr_g2_unc, tpr_g2 - tpr_g2_unc, alpha=0.2)\n",
    "\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xlabel('$\\gamma$')\n",
    "ax2.set_ylabel('TPR')\n",
    "\n",
    "savefig('gender_constrained_pareto.svg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By investigating the pareto curves empirically, we see that $\\gamma = 0.005$ achieves the best tradeoff between performance and fairness. We train a new model with this gamma value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_fair = KfoldTracker()\n",
    "\n",
    "gamma = 5e-3\n",
    "\n",
    "n_splits = 10\n",
    "splits = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(income)):\n",
    "    train = income.iloc[train_idx]\n",
    "    val = income.iloc[val_idx]\n",
    "    \n",
    "    x_train = preprocessing.fit_transform(train).toarray()\n",
    "    y_train = train[target].values\n",
    "    g_train = train[groups].values\n",
    "    \n",
    "    x_val = preprocessing.transform(val).toarray()\n",
    "    y_val = val[target].values\n",
    "    g_val = val[groups].values\n",
    "\n",
    "    x_train, y_train, g_train, x_val, y_val, g_val = map(lambda x: torch.tensor(x, dtype=torch.float), (x_train, y_train, g_train, x_val, y_val, g_val))\n",
    "\n",
    "    model = FairBinaryLogisticRegression(x_train.shape[1], gamma=gamma)\n",
    "    model.fit(x_train, y_train, g_train[:, 0])\n",
    "    \n",
    "    y_pred = model(x_val)\n",
    "    y_pred = y_pred.detach().numpy().round()\n",
    "\n",
    "    feature_importance = torch.exp(model.linear.weight).detach().numpy().squeeze()    \n",
    "    feature_names = preprocessing.get_feature_names_out()\n",
    "    tracker_fair.log(y_val, y_pred, g_val, feature_importance, feature_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model, we evaluate the performance, fairness and feature importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_fair = tracker_fair.get_mean_performance()\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(perf_fair.keys(), [v[0] for v in perf_fair.values()], yerr=[v[1] for v in perf_fair.values()])\n",
    "ax.set_title('Adult income - gender constrained model\\nPerformance')\n",
    "ax.set_ylabel('Score')\n",
    "savefig('constrained_performance.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plot_outcomes(tracker_fair)\n",
    "\n",
    "fig.suptitle('Adult income - gender constrained model\\nClassification outcome')\n",
    "savefig('constrained_fairness.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_fair = tracker_fair.get_mean_feature_importance()\n",
    "fig, (ax1, ax2) = plot_feature_importance(feature_importance_fair)\n",
    "ax2.set_xlabel('Feature importance')\n",
    "fig.suptitle('Adult income - gender constrained model\\nFeature importance for income > 50k')\n",
    "savefig('constrained_feature_importance.svg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application of method on 3 protected groups\n",
    "We repeat the experiment from before, but now we do it in a more realistic manner; we apply the group fairness penalty on 3 groups under optimization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a model that supports fairness regularization on any number of groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VeryFairBinaryLogisticRegression(BinaryLogisticRegression):\n",
    "    def __init__(self, in_dim: int, gamma: float = 1e-4):\n",
    "        super().__init__(in_dim)\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def fit(self, x, y, gs, lr=0.01, weight_decay=0.01, batch_size=128, n_iter=10, verbose=False):\n",
    "        criterion = nn.BCELoss()\n",
    "        fair_criterion = GroupFairnessConstraint()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        trainloader = DataLoader(TensorDataset(x, y, gs), batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        for epoch in range(n_iter):\n",
    "            for x, y, g in trainloader:\n",
    "                y_pred = self(x)\n",
    "                loss_bce = criterion(y_pred, y)\n",
    "                loss_fair = sum([self.gamma * fair_criterion(y_pred, y, g[:, idx]) for idx in range(g.shape[1])])\n",
    "                loss = loss_bce + loss_fair\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if verbose:\n",
    "                    print(f'Epoch: {epoch}, Loss BCE: {round(loss_bce.item(), 4)}, Loss Fair: {round(loss_fair.item(), 4)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gridsearch to find the optimal regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammaspace = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1]\n",
    "trackers = []\n",
    "\n",
    "for gamma in gammaspace:\n",
    "    tracker = KfoldTracker()\n",
    "\n",
    "    n_splits = 5\n",
    "    splits = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    for fold, (train_idx, val_idx) in enumerate(splits.split(income)):\n",
    "        train = income.iloc[train_idx]\n",
    "        val = income.iloc[val_idx]\n",
    "        \n",
    "        x_train = preprocessing.fit_transform(train).toarray()\n",
    "        y_train = train[target].values\n",
    "        g_train = train[groups].values\n",
    "        \n",
    "        x_val = preprocessing.transform(val).toarray()\n",
    "        y_val = val[target].values\n",
    "        g_val = val[groups].values\n",
    "\n",
    "        x_train, y_train, g_train, x_val, y_val, g_val = map(lambda x: torch.tensor(x, dtype=torch.float), (x_train, y_train, g_train, x_val, y_val, g_val))\n",
    "\n",
    "        model = VeryFairBinaryLogisticRegression(x_train.shape[1], gamma=gamma)\n",
    "        model.fit(x_train, y_train, g_train)\n",
    "        \n",
    "        y_pred = model(x_val)\n",
    "        y_pred = y_pred.detach().numpy().round()\n",
    "\n",
    "        feature_importance = torch.exp(model.linear.weight).detach().numpy().squeeze()    \n",
    "        feature_names = preprocessing.get_feature_names_out()\n",
    "        tracker.log(y_val, y_pred, g_val, feature_importance, feature_names)\n",
    "\n",
    "        mask = g_val[:, 0] == 0\n",
    "        tracker['male'].log_performance(y_val[mask], y_pred[mask])\n",
    "        tracker['female'].log_performance(y_val[~mask], y_pred[~mask])\n",
    "\n",
    "        mask = g_val[:, 1] == 0\n",
    "        tracker['white'].log_performance(y_val[mask], y_pred[mask])\n",
    "        tracker['other'].log_performance(y_val[~mask], y_pred[~mask])\n",
    "\n",
    "        mask = g_val[:, 2] == 0\n",
    "        tracker['<35'].log_performance(y_val[mask], y_pred[mask])\n",
    "        tracker['>=35'].log_performance(y_val[~mask], y_pred[~mask])\n",
    "\n",
    "    trackers.append(tracker)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the f1-score and tpr of the models with different regularization strengths in a pareto curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_perf = np.array([t['male'].get_mean_performance()['f1-score'][0] for t in trackers])\n",
    "female_perf = np.array([t['female'].get_mean_performance()['f1-score'][0] for t in trackers])\n",
    "male_unc = np.array([t['male'].get_mean_performance()['f1-score'][1] for t in trackers])\n",
    "female_unc = np.array([t['female'].get_mean_performance()['f1-score'][1] for t in trackers])\n",
    "\n",
    "white_perf = np.array([t['white'].get_mean_performance()['f1-score'][0] for t in trackers])\n",
    "other_perf = np.array([t['other'].get_mean_performance()['f1-score'][0] for t in trackers])\n",
    "white_unc = np.array([t['white'].get_mean_performance()['f1-score'][1] for t in trackers])\n",
    "other_unc = np.array([t['other'].get_mean_performance()['f1-score'][1] for t in trackers])\n",
    "\n",
    "young_perf = np.array([t['<35'].get_mean_performance()['f1-score'][0] for t in trackers])\n",
    "old_perf = np.array([t['>=35'].get_mean_performance()['f1-score'][0] for t in trackers])\n",
    "young_unc = np.array([t['<35'].get_mean_performance()['f1-score'][1] for t in trackers])\n",
    "old_unc = np.array([t['>=35'].get_mean_performance()['f1-score'][1] for t in trackers])\n",
    "\n",
    "gender_tpr_g1 = np.array([t['group_0'].get_mean_fairness()['TPR G1'][0] for t in trackers])\n",
    "gender_tpr_g2 = np.array([t['group_0'].get_mean_fairness()['TPR G2'][0] for t in trackers])\n",
    "gender_tpr_g1_unc = np.array([t['group_0'].get_mean_fairness()['TPR G1'][1] for t in trackers])\n",
    "gender_tpr_g2_unc = np.array([t['group_0'].get_mean_fairness()['TPR G2'][1] for t in trackers])\n",
    "\n",
    "race_tpr_g1 = np.array([t['group_1'].get_mean_fairness()['TPR G1'][0] for t in trackers])\n",
    "race_tpr_g2 = np.array([t['group_1'].get_mean_fairness()['TPR G2'][0] for t in trackers])\n",
    "race_tpr_g1_unc = np.array([t['group_1'].get_mean_fairness()['TPR G1'][1] for t in trackers])\n",
    "race_tpr_g2_unc = np.array([t['group_1'].get_mean_fairness()['TPR G2'][1] for t in trackers])\n",
    "\n",
    "age_tpr_g1 = np.array([t['group_2'].get_mean_fairness()['TPR G1'][0] for t in trackers])\n",
    "age_tpr_g2 = np.array([t['group_2'].get_mean_fairness()['TPR G2'][0] for t in trackers])\n",
    "age_tpr_g1_unc = np.array([t['group_2'].get_mean_fairness()['TPR G1'][1] for t in trackers])\n",
    "age_tpr_g2_unc = np.array([t['group_2'].get_mean_fairness()['TPR G2'][1] for t in trackers])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "ax1.plot(gammaspace, male_perf, label='male')\n",
    "ax1.plot(gammaspace, female_perf, label='female')\n",
    "\n",
    "ax1.fill_between(gammaspace, male_perf + male_unc, male_perf - male_unc, alpha=0.2)\n",
    "ax1.fill_between(gammaspace, female_perf + female_unc, female_perf - female_unc, alpha=0.2)\n",
    "\n",
    "ax1.plot(gammaspace, white_perf, label='white')\n",
    "ax1.plot(gammaspace, other_perf, label='other race')\n",
    "\n",
    "ax1.fill_between(gammaspace, white_perf + white_unc, white_perf - white_unc, alpha=0.2)\n",
    "ax1.fill_between(gammaspace, other_perf + other_unc, other_perf - other_unc, alpha=0.2)\n",
    "\n",
    "ax1.plot(gammaspace, young_perf, label='young')\n",
    "ax1.plot(gammaspace, old_perf, label='old')\n",
    "\n",
    "ax1.fill_between(gammaspace, young_perf + young_unc, young_perf - young_unc, alpha=0.2)\n",
    "ax1.fill_between(gammaspace, old_perf + old_unc, old_perf - old_unc, alpha=0.2)\n",
    "\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "ax1.set_ylabel('F1-score')\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "ax1.set_title('Adult income - triple constrained model\\nPareto curves for different values of gamma')\n",
    "\n",
    "ax2.plot(gammaspace, gender_tpr_g1, label='TPR G1')\n",
    "ax2.plot(gammaspace, gender_tpr_g2, label='TPR G2')\n",
    "ax2.fill_between(gammaspace, gender_tpr_g1 + gender_tpr_g1_unc, gender_tpr_g1 - gender_tpr_g1_unc, alpha=0.2)\n",
    "ax2.fill_between(gammaspace, gender_tpr_g2 + gender_tpr_g2_unc, gender_tpr_g2 - gender_tpr_g2_unc, alpha=0.2)\n",
    "\n",
    "ax2.plot(gammaspace, race_tpr_g1, label='TPR G1')\n",
    "ax2.plot(gammaspace, race_tpr_g2, label='TPR G2')\n",
    "ax2.fill_between(gammaspace, race_tpr_g1 + race_tpr_g1_unc, race_tpr_g1 - race_tpr_g1_unc, alpha=0.2)\n",
    "ax2.fill_between(gammaspace, race_tpr_g2 + race_tpr_g2_unc, race_tpr_g2 - race_tpr_g2_unc, alpha=0.2)\n",
    "\n",
    "ax2.plot(gammaspace, age_tpr_g1, label='TPR G1')\n",
    "ax2.plot(gammaspace, age_tpr_g2, label='TPR G2')\n",
    "ax2.fill_between(gammaspace, age_tpr_g1 + age_tpr_g1_unc, age_tpr_g1 - age_tpr_g1_unc, alpha=0.2)\n",
    "ax2.fill_between(gammaspace, age_tpr_g2 + age_tpr_g2_unc, age_tpr_g2 - age_tpr_g2_unc, alpha=0.2)\n",
    "\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xlabel('$\\gamma$')\n",
    "ax2.set_ylabel('TPR')\n",
    "\n",
    "savefig('pareto_curves_triple_constrained.svg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating the curve, we see the best accuracy-fairness tradeoff is at $\\gamma=0.005$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_3 = KfoldTracker()\n",
    "\n",
    "gamma = 5e-3\n",
    "\n",
    "n_splits = 10\n",
    "splits = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "for fold, (train_idx, val_idx) in enumerate(splits.split(income)):\n",
    "    train = income.iloc[train_idx]\n",
    "    val = income.iloc[val_idx]\n",
    "    \n",
    "    x_train = preprocessing.fit_transform(train).toarray()\n",
    "    y_train = train[target].values\n",
    "    g_train = train[groups].values\n",
    "    \n",
    "    x_val = preprocessing.transform(val).toarray()\n",
    "    y_val = val[target].values\n",
    "    g_val = val[groups].values\n",
    "\n",
    "    x_train, y_train, g_train, x_val, y_val, g_val = map(lambda x: torch.tensor(x, dtype=torch.float), (x_train, y_train, g_train, x_val, y_val, g_val))\n",
    "\n",
    "    model = VeryFairBinaryLogisticRegression(x_train.shape[1], gamma=gamma)\n",
    "    model.fit(x_train, y_train, g_train)\n",
    "    \n",
    "    y_pred = model(x_val)\n",
    "    y_pred = y_pred.detach().numpy().round()\n",
    "\n",
    "    feature_importance = torch.exp(model.linear.weight).detach().numpy().squeeze()    \n",
    "    feature_names = preprocessing.get_feature_names_out()\n",
    "    tracker_3.log(y_val, y_pred, g_val, feature_importance, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_3 = tracker_3.get_mean_performance()\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(perf_3.keys(), [v[0] for v in perf_3.values()], yerr=[v[1] for v in perf_3.values()])\n",
    "ax.set_title('Adult income - triple constrained model\\nPerformance')\n",
    "ax.set_ylabel('Score')\n",
    "savefig('triple_constrained_performance.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plot_outcomes(tracker_3)\n",
    "fig.suptitle('Adult income - triple constrained model\\nOutcomes')\n",
    "savefig('triple_constrained_outcomes.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_3 = tracker_3.get_mean_feature_importance()\n",
    "fig, (ax1, ax2) = plot_feature_importance(feature_importance_3)\n",
    "ax2.set_xlabel('Feature importance')\n",
    "fig.suptitle('Adult income - triple constrained model\\nFeature importance for income > 50k')\n",
    "savefig('triple_constrained_feature_importance.svg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison across all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "categories = np.array(list(tracker_base.get_mean_performance().keys()))\n",
    "\n",
    "perf_base = [v[0] for v in tracker_base.get_mean_performance().values()]\n",
    "perf_fair = [v[0] for v in tracker_fair.get_mean_performance().values()]\n",
    "perf_3 = [v[0] for v in tracker_3.get_mean_performance().values()]\n",
    "\n",
    "unc_base = [v[1] for v in tracker_base.get_mean_performance().values()]\n",
    "unc_fair = [v[1] for v in tracker_fair.get_mean_performance().values()]\n",
    "unc_3 = [v[1] for v in tracker_3.get_mean_performance().values()]\n",
    "\n",
    "x_axis = np.arange(len(categories))\n",
    "ax.bar(x_axis-.3, perf_base, yerr=unc_base, width=.3, label='Baseline')\n",
    "ax.bar(x_axis, perf_fair, yerr=unc_fair, width=.3, label='Gender constrained')\n",
    "ax.bar(x_axis+.3, perf_3, yerr=unc_3, width=.3, label='Triple constrained')\n",
    "\n",
    "ax.set_xticks(x_axis)\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_ylabel('Score')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_title('Adult income\\nComparison of performance')\n",
    "savefig('performance_comparison.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, sharey=True, figsize=(14, 6))\n",
    "bin_width = .3\n",
    "\n",
    "x_axis = np.arange(len(tracker_base[f'group_{0}'].get_mean_fairness().keys()))\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    fairness = tracker_base[f'group_{i}'].get_mean_fairness()\n",
    "    axs[i].bar(x_axis - bin_width, [v[0] for v in fairness.values()], yerr=[v[1] for v in fairness.values()], width=bin_width, color=red, label='Baseline')\n",
    "    axs[i].set_title(f'{groups[i]}'.replace('_', ' '))\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    fairness = tracker_fair[f'group_{i}'].get_mean_fairness()\n",
    "    axs[i].bar(x_axis, [v[0] for v in fairness.values()], yerr=[v[1] for v in fairness.values()], width=bin_width, color=blue, label='Gender constrained')\n",
    "    axs[i].set_title(f'{groups[i]}'.replace('_', ' '))\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    fairness = tracker_3[f'group_{i}'].get_mean_fairness()\n",
    "    axs[i].bar(x_axis + bin_width, [v[0] for v in fairness.values()], yerr=[v[1] for v in fairness.values()], width=bin_width, color=purple, label='Triple constrained')\n",
    "    axs[i].set_title(f'{groups[i]}'.replace('_', ' '))\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xticks([0, 1, 2, 3])\n",
    "    ax.set_xticklabels(tracker_base[f'group_{0}'].get_mean_fairness().keys())\n",
    "\n",
    "axs[0].set_ylabel('Ratio')\n",
    "fig.suptitle('Adult income\\nComparison of fairness')\n",
    "savefig('fairness_comparison.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = np.array(list(feature_importance_base.keys()))\n",
    "values_base = np.array([v[0] for v in feature_importance_base.values()])\n",
    "values_fair = np.array([v[0] for v in feature_importance_fair.values()])\n",
    "values_3 = np.array([v[0] for v in feature_importance_3.values()])\n",
    "uncertainty_base = np.array([v[1] for v in feature_importance_base.values()])\n",
    "uncertainty_fair = np.array([v[1] for v in feature_importance_fair.values()])\n",
    "uncertainty_3 = np.array([v[1] for v in feature_importance_3.values()])\n",
    "\n",
    "categories = np.array([cat.split('__')[1] if 'onehotencoder' in cat else cat for cat in categories])\n",
    "\n",
    "idx = np.argsort(values_base)\n",
    "categories, values_base, values_fair, values_3, uncertainty_base, uncertainty_fair, uncertainty_3 = map(lambda x: x[idx], (categories, values_base, values_fair, values_3, uncertainty_base, uncertainty_fair, uncertainty_3))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 12))\n",
    "\n",
    "for i in range(len(categories)):\n",
    "    ax.plot(values_base[i], i, 'o', color=red, label='Baseline' if i == 0 else None)\n",
    "    ax.plot(values_fair[i], i, 'o', color=purple, label='Gender constrained' if i == 0 else None)\n",
    "    ax.plot(values_3[i], i, 'o', color=blue, label='Triple constrained' if i == 0 else None)\n",
    "    ax.fill_betweenx([i-0.33, i+0.33], values_base[i]-uncertainty_base[i], values_base[i]+uncertainty_base[i], color=red, alpha=0.4)\n",
    "    ax.fill_betweenx([i-0.33, i+0.33], values_fair[i]-uncertainty_fair[i], values_fair[i]+uncertainty_fair[i], color=purple, alpha=0.4)\n",
    "    ax.fill_betweenx([i-0.33, i+0.33], values_3[i]-uncertainty_3[i], values_3[i]+uncertainty_3[i], color=blue, alpha=0.4)\n",
    "\n",
    "ax.set_yticks(range(len(categories)))\n",
    "ax.set_yticklabels(categories)\n",
    "\n",
    "ax.set_xlabel('Feature importance')\n",
    "\n",
    "ax.set_title('Adult income\\nComparison of feature importance')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "savefig('feature_importance_comparison.svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
